{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21d3b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c27c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "spark = SparkSession.builder.appName('kppysparkwindowfunc').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caff95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleData = ((\"James\", \"Sales\", 4000),\n",
    "              (\"Michael\", \"Sales\", 4500),\n",
    "              (\"Robert\", \"Marketing\", 4100),\n",
    "              (\"Maria\", \"Finance\", 5600),\n",
    "              (\"James\", \"Sales\", 3000),\n",
    "              (\"Scott\", \"Finance\", 4500))\n",
    "columns = [\"employee_name\",\"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = sampleData, schema=columns)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17a530e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |4000  |\n",
      "|Michael      |Sales     |4500  |\n",
      "|Robert       |Marketing |4100  |\n",
      "|Maria        |Finance   |5600  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |4500  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5b0eb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|Scott        |Finance   |4500  |1         |\n",
      "|Maria        |Finance   |5600  |2         |\n",
      "|Robert       |Marketing |4100  |1         |\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |4000  |2         |\n",
      "|Michael      |Sales     |4500  |3         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
    "df.withColumn(\"row_number\", row_number().over(windowSpec)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b98f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        Scott|   Finance|  4500|   1|\n",
      "|        Maria|   Finance|  5600|   2|\n",
      "|       Robert| Marketing|  4100|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  4000|   2|\n",
      "|      Michael|     Sales|  4500|   3|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\", rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e27c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        Scott|   Finance|  4500|         1|\n",
      "|        Maria|   Finance|  5600|         2|\n",
      "|       Robert| Marketing|  4100|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  4000|         2|\n",
      "|      Michael|     Sales|  4500|         3|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank \n",
    "df.withColumn(\"dense_rank\", dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f94b6600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        Scott|   Finance|  4500|         0.0|\n",
      "|        Maria|   Finance|  5600|         1.0|\n",
      "|       Robert| Marketing|  4100|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  4000|         0.5|\n",
      "|      Michael|     Sales|  4500|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\", percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3516e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        Scott|   Finance|  4500|    1|\n",
      "|        Maria|   Finance|  5600|    2|\n",
      "|       Robert| Marketing|  4100|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  4000|    1|\n",
      "|      Michael|     Sales|  4500|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ntile() is a window function which distributes rows into a ordered partition if a pre-defined number of roughly equal groups. \n",
    "# it assigns each group a pre-defined number of roughly equal group with a number expression ranging from 1. \n",
    "# ntile() assigns a number_expression for every row in a group in which the row belongs. \n",
    "\n",
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\", ntile(2).over(windowSpec)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c41b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|         cume_dist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        Scott|   Finance|  4500|               0.5|\n",
      "|        Maria|   Finance|  5600|               1.0|\n",
      "|       Robert| Marketing|  4100|               1.0|\n",
      "|        James|     Sales|  3000|0.3333333333333333|\n",
      "|        James|     Sales|  4000|0.6666666666666666|\n",
      "|      Michael|     Sales|  4500|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the cume_dist() function returns in SQL the cumulative distribution of a value within a group of values. It calculates\n",
    "# the relative position of a value in a group of values. \n",
    "\n",
    "from pyspark.sql.functions import cume_dist\n",
    "df.withColumn(\"cume_dist\", cume_dist().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "633e7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate functions in PySpark SQL \n",
    "\n",
    "# row_number(), min(), max(), avg(), sum() etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c32cedb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+----+----+\n",
      "|department|               avg|               sum| min| max|\n",
      "+----------+------------------+------------------+----+----+\n",
      "|   Finance|            5050.0|            5050.0|4500|5600|\n",
      "| Marketing|            4100.0|            4100.0|4100|4100|\n",
      "|     Sales|3833.3333333333335|3833.3333333333335|3000|4500|\n",
      "+----------+------------------+------------------+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg = Window.partitionBy(\"department\")\n",
    "from pyspark.sql.functions import col, avg, sum, min, max, row_number\n",
    "df.withColumn(\"row\", row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\",avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\",avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\",min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\", \"max\").show()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7531c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
